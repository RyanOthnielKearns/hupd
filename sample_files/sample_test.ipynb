{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvard USPTO Patent Dataset (HUPD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Sample Dataset, stored locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import relevant libraries and dependencies\n",
    "# Pretty print\n",
    "from pprint import pprint\n",
    "# Datasets load_dataset function\n",
    "from datasets import load_dataset\n",
    "# Transformers Autokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# Standard PyTorch DataLoader\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `load_dataset` function to load all the patent applications that were filed to the USPTO in January 2016. We specify the date ranges of the training and validation sets as January 1-21, 2016 and January 22-31, 2016, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6c866e603216e695\n",
      "Reusing dataset patents (./mnt/data/HUPD/cache/patents/default-6c866e603216e695/1.0.1/0d005d4e2200f89fac8ee7f637a6c4ad0ec749df3f747d586c3e015e0be324b4)\n",
      "100%|██████████| 2/2 [00:00<00:00, 581.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading is done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Data loading example\n",
    "dataset_dict = load_dataset('./sample_dataset.py', \n",
    "    data_dir='./sample_json',\n",
    "    metadata_file=\"sample_metadata.feather\",\n",
    "    cache_dir='./mnt/data/HUPD/cache',\n",
    "    ipcr_label=None,\n",
    "    uniform_split=True\n",
    ")\n",
    "\n",
    "print('Loading is done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display some information about the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['patent_number', 'decision', 'title', 'abstract', 'claims', 'background', 'summary', 'description', 'cpc_label', 'ipc_label', 'filing_date', 'patent_issue_date', 'date_published', 'examiner_id'],\n",
      "        num_rows: 8\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['patent_number', 'decision', 'title', 'abstract', 'claims', 'background', 'summary', 'description', 'cpc_label', 'ipc_label', 'filing_date', 'patent_issue_date', 'date_published', 'examiner_id'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Dataset info\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also display the fields within the dataset dictionary, as well as the sizes of the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dictionary contents:\n",
      "{'train': Dataset({\n",
      "    features: ['patent_number', 'decision', 'title', 'abstract', 'claims', 'background', 'summary', 'description', 'cpc_label', 'ipc_label', 'filing_date', 'patent_issue_date', 'date_published', 'examiner_id'],\n",
      "    num_rows: 8\n",
      "}),\n",
      " 'validation': Dataset({\n",
      "    features: ['patent_number', 'decision', 'title', 'abstract', 'claims', 'background', 'summary', 'description', 'cpc_label', 'ipc_label', 'filing_date', 'patent_issue_date', 'date_published', 'examiner_id'],\n",
      "    num_rows: 1\n",
      "})}\n",
      "Dataset dictionary cached to:\n",
      "{'train': [{'filename': './mnt/data/HUPD/cache/patents/default-6c866e603216e695/1.0.1/0d005d4e2200f89fac8ee7f637a6c4ad0ec749df3f747d586c3e015e0be324b4/patents-train.arrow'}],\n",
      " 'validation': [{'filename': './mnt/data/HUPD/cache/patents/default-6c866e603216e695/1.0.1/0d005d4e2200f89fac8ee7f637a6c4ad0ec749df3f747d586c3e015e0be324b4/patents-validation.arrow'}]}\n"
     ]
    }
   ],
   "source": [
    "# Print dataset dictionary contents and cache directory\n",
    "print('Dataset dictionary contents:')\n",
    "pprint(dataset_dict)\n",
    "print('Dataset dictionary cached to:')\n",
    "pprint(dataset_dict.cache_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: (8, 14)\n",
      "Validation dataset size: (1, 14)\n"
     ]
    }
   ],
   "source": [
    "# Print info about the sizes of the train and validation sets\n",
    "print(f'Train dataset size: {dataset_dict[\"train\"].shape}')\n",
    "print(f'Validation dataset size: {dataset_dict[\"validation\"].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's establish the label-to-index mapping for the decision status field by assigning the decision status labels to the class indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label-to-index mapping for the decision status field\n",
    "decision_to_str = {'REJECTED': 0, 'ACCEPTED': 1, 'PENDING': 2, 'CONT-REJECTED': 3, 'CONT-ACCEPTED': 4, 'CONT-PENDING': 5}\n",
    "\n",
    "# Helper function\n",
    "def map_decision_to_string(example):\n",
    "    return {'decision': decision_to_str[example['decision']]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now re-label the decision status fields of the examples in the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8ex [00:00, 2655.04ex/s]\n",
      "1ex [00:00, 845.97ex/s]\n"
     ]
    }
   ],
   "source": [
    "# Re-labeling/mapping.\n",
    "train_set = dataset_dict['train'].map(map_decision_to_string)\n",
    "val_set = dataset_dict['validation'].map(map_decision_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed train and validation sets are cached to: \n",
      "[{'filename': './mnt/data/HUPD/cache/patents/default-4ab836bc029274ca/1.0.1/0d005d4e2200f89fac8ee7f637a6c4ad0ec749df3f747d586c3e015e0be324b4/cache-4a54da1ef066ec70.arrow'}]\n",
      "[{'filename': './mnt/data/HUPD/cache/patents/default-4ab836bc029274ca/1.0.1/0d005d4e2200f89fac8ee7f637a6c4ad0ec749df3f747d586c3e015e0be324b4/cache-c1ce181364dc69f4.arrow'}]\n"
     ]
    }
   ],
   "source": [
    "# Display the cached directories of the processed train and validation sets\n",
    "print('Processed train and validation sets are cached to: ')\n",
    "pprint(train_set.cache_files)\n",
    "pprint(val_set.cache_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the time being, let's focus on the _abstract_ section of the patent applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on the abstract section and tokenize the text using the tokenizer. \n",
    "_SECTION_ = 'abstract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 46.30ba/s]\n"
     ]
    }
   ],
   "source": [
    "# Training set\n",
    "train_set = train_set.map(\n",
    "    lambda e: tokenizer((e[_SECTION_]), truncation=True, padding='max_length'),\n",
    "    batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 187.16ba/s]\n"
     ]
    }
   ],
   "source": [
    "# Validation set\n",
    "val_set = val_set.map(\n",
    "    lambda e: tokenizer((e[_SECTION_]), truncation=True, padding='max_length'),\n",
    "    batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the format\n",
    "train_set.set_format(type='torch', \n",
    "    columns=['input_ids', 'attention_mask', 'decision'])\n",
    "\n",
    "val_set.set_format(type='torch', \n",
    "    columns=['input_ids', 'attention_mask', 'decision'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `DataLoader` to crete our training set and validation set loaders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader and val_data_loader\n",
    "train_dataloader = DataLoader(train_set, batch_size=16)\n",
    "val_dataloader = DataLoader(val_set, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1037,  4118,  ...,     0,     0,     0],\n",
      "        [  101,  1037, 11394,  ...,     0,     0,     0],\n",
      "        [  101,  2004, 27108,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  1037, 14513,  ...,     0,     0,     0],\n",
      "        [  101,  1037, 10808,  ...,     0,     0,     0],\n",
      "        [  101,  1996, 11028,  ...,     0,     0,     0]])\n",
      "tensor([1, 4, 0, 1, 3, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# Get the next batch\n",
    "batch = next(iter(train_dataloader))\n",
    "# Print the ids\n",
    "pprint(batch['input_ids'])\n",
    "# Print the labels\n",
    "pprint(batch['decision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([8, 512])\n",
      "Output shape: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# Print the input and output shapes\n",
    "input_shape = batch['input_ids'].shape\n",
    "output_shape = batch['decision'].shape\n",
    "print(f'Input shape: {input_shape}')\n",
    "print(f'Output shape: {output_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function that converts ids into tokens\n",
    "def convert_ids_to_string(tokenizer, input):\n",
    "    return ' '.join(tokenizer.convert_ids_to_tokens(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print an example in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS] a dental filling material comprising a the ##rm ##op ##lastic polymer '\n",
      " '. the the ##rm ##op ##lastic polymer may be bio ##de ##grad ##able . a bio '\n",
      " '##active substance may also be included in the filling material . the the '\n",
      " '##rm ##op ##lastic polymer acts as a matrix for the bio ##active substance . '\n",
      " 'the composition may include other polymer ##ic resin ##s , fill ##ers , '\n",
      " 'plastic ##izer ##s and other additive ##s typically used in dental materials '\n",
      " '. the filling material is used for the filing of root canals . [SEP] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]')\n"
     ]
    }
   ],
   "source": [
    "# Print the example\n",
    "pprint(convert_ids_to_string(tokenizer,batch['input_ids'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('patents_shannon': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b04fe1312419ea669896150c5e116bb477c9aa479dfef38c921e951b90dadf16"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
