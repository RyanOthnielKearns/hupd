{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvard USPTO Patent Dataset (HUPD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset Using Hugging Face's Datasets and Transformers Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we show how to load and use the HUPD using Hugging Face's Datasets and Transformers libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import relevant libraries and dependencies\n",
    "# Pretty print\n",
    "from pprint import pprint\n",
    "# Datasets load_dataset function\n",
    "from datasets import load_dataset\n",
    "# Transformers Autokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# Standard PyTorch DataLoader\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `load_dataset` function to load all the patent applications that were filed to the USPTO in January 2016. We specify the date ranges of the training and validation sets as January 1-21, 2016 and January 22-31, 2016, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset patents (/mnt/data/HUPD/cache/patents/default-f6746976a4961295/1.0.1/704348b414e8c2991a15841dda7af72c4d35249bc4c98b06c41e6deb8b2367e8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading is done!\n"
     ]
    }
   ],
   "source": [
    "# Data loading example\n",
    "dataset_dict = load_dataset('/mnt/data/HUPD/patents-project-dataset/datasets/patents/patents.py', \n",
    "    data_dir='/mnt/data/HUPD/distilled',\n",
    "    cache_dir='/mnt/data/HUPD/cache',\n",
    "    icpr_label=None,\n",
    "    train_filing_start_date='2016-01-01',\n",
    "    train_filing_end_date='2016-01-21',\n",
    "    val_filing_start_date='2016-01-22',\n",
    "    val_filing_end_date='2016-01-31',\n",
    ")\n",
    "\n",
    "print('Loading is done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display some information about the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['patent_number', 'decision', 'title', 'abstract', 'claims', 'background', 'summary', 'description', 'cpc_label', 'ipc_label', 'filing_date', 'patent_issue_date', 'date_published', 'examiner_id'],\n",
      "        num_rows: 17614\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['patent_number', 'decision', 'title', 'abstract', 'claims', 'background', 'summary', 'description', 'cpc_label', 'ipc_label', 'filing_date', 'patent_issue_date', 'date_published', 'examiner_id'],\n",
      "        num_rows: 9194\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Dataset info\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also display the fields within the dataset dictionary, as well as the sizes of the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dictionary contents:\n",
      "{'train': Dataset({\n",
      "    features: ['patent_number', 'decision', 'title', 'abstract', 'claims', 'background', 'summary', 'description', 'cpc_label', 'ipc_label', 'filing_date', 'patent_issue_date', 'date_published', 'examiner_id'],\n",
      "    num_rows: 17614\n",
      "}),\n",
      " 'validation': Dataset({\n",
      "    features: ['patent_number', 'decision', 'title', 'abstract', 'claims', 'background', 'summary', 'description', 'cpc_label', 'ipc_label', 'filing_date', 'patent_issue_date', 'date_published', 'examiner_id'],\n",
      "    num_rows: 9194\n",
      "})}\n",
      "Dataset dictionary cached to:\n",
      "{'train': [{'filename': '/mnt/data/HUPD/cache/patents/default-f6746976a4961295/1.0.1/704348b414e8c2991a15841dda7af72c4d35249bc4c98b06c41e6deb8b2367e8/patents-train.arrow',\n",
      "            'skip': 0,\n",
      "            'take': 17614}],\n",
      " 'validation': [{'filename': '/mnt/data/HUPD/cache/patents/default-f6746976a4961295/1.0.1/704348b414e8c2991a15841dda7af72c4d35249bc4c98b06c41e6deb8b2367e8/patents-validation.arrow',\n",
      "                 'skip': 0,\n",
      "                 'take': 9194}]}\n"
     ]
    }
   ],
   "source": [
    "# Print dataset dictionary contents and cache directory\n",
    "print('Dataset dictionary contents:')\n",
    "pprint(dataset_dict)\n",
    "print('Dataset dictionary cached to:')\n",
    "pprint(dataset_dict.cache_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: (17614, 14)\n",
      "Validation dataset size: (9194, 14)\n"
     ]
    }
   ],
   "source": [
    "# Print info about the sizes of the train and validation sets\n",
    "print(f'Train dataset size: {dataset_dict[\"train\"].shape}')\n",
    "print(f'Validation dataset size: {dataset_dict[\"validation\"].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's establish the label-to-index mapping for the decision status field by assigning the decision status labels to the class indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label-to-index mapping for the decision status field\n",
    "decision_to_str = {'REJECTED': 0, 'ACCEPTED': 1, 'PENDING': 2, 'CONT-REJECTED': 3, 'CONT-ACCEPTED': 4, 'CONT-PENDING': 5}\n",
    "\n",
    "# Helper function\n",
    "def map_decision_to_string(example):\n",
    "    return {'decision': decision_to_str[example['decision']]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now re-label the decision status fields of the examples in the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0c9ecf55da4c588f86f59c50fc049f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=17614.0), HTML(value='')))"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1ef01f1a2d431da6601f9d9532cc14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=9194.0), HTML(value='')))"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Re-labeling/mapping.\n",
    "train_set = dataset_dict['train'].map(map_decision_to_string)\n",
    "val_set = dataset_dict['validation'].map(map_decision_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed train and validation sets are cached to: \n",
      "[{'filename': '/mnt/data/HUPD/cache/patents/default-f6746976a4961295/1.0.1/704348b414e8c2991a15841dda7af72c4d35249bc4c98b06c41e6deb8b2367e8/cache-96136822676cc6f9.arrow'}]\n",
      "[{'filename': '/mnt/data/HUPD/cache/patents/default-f6746976a4961295/1.0.1/704348b414e8c2991a15841dda7af72c4d35249bc4c98b06c41e6deb8b2367e8/cache-a5514eba224d4829.arrow'}]\n"
     ]
    }
   ],
   "source": [
    "# Display the cached directories of the processed train and validation sets\n",
    "print('Processed train and validation sets are cached to: ')\n",
    "pprint(train_set.cache_files)\n",
    "pprint(val_set.cache_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the time being, let's focus on the _abstract_ section of the patent applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on the abstract section and tokenize the text using the tokenizer. \n",
    "_SECTION_ = 'abstract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27196a94b7b249798b7cfe8143062d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=18.0), HTML(value='')))"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training set\n",
    "train_set = train_set.map(\n",
    "    lambda e: tokenizer((e[_SECTION_]), truncation=True, padding='max_length'),\n",
    "    batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff476e61c9bb4c3e8e35ddf01d15cbbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
     },
     "metadata": {
      "transient": {}
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Validation set\n",
    "val_set = val_set.map(\n",
    "    lambda e: tokenizer((e[_SECTION_]), truncation=True, padding='max_length'),\n",
    "    batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the format\n",
    "train_set.set_format(type='torch', \n",
    "    columns=['input_ids', 'attention_mask', 'decision'])\n",
    "\n",
    "val_set.set_format(type='torch', \n",
    "    columns=['input_ids', 'attention_mask', 'decision'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `DataLoader` to crete our training set and validation set loaders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader and val_data_loader\n",
    "train_dataloader = DataLoader(train_set, batch_size=16)\n",
    "val_dataloader = DataLoader(val_set, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1996,  2556,  ...,     0,     0,     0],\n",
      "        [  101,  7861,  5092,  ...,     0,     0,     0],\n",
      "        [  101,  1037, 12109,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  5622,  4226,  ...,     0,     0,     0],\n",
      "        [  101,  1037,  3259,  ...,     0,     0,     0],\n",
      "        [  101,  1996,  2556,  ...,     0,     0,     0]])\n",
      "tensor([1, 1, 2, 1, 0, 1, 0, 1, 0, 1, 2, 1, 1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Get the next batch\n",
    "batch = next(iter(train_dataloader))\n",
    "# Print the ids\n",
    "pprint(batch['input_ids'])\n",
    "# Print the labels\n",
    "pprint(batch['decision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([16, 512])\n",
      "Output shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# Print the input and output shapes\n",
    "input_shape = batch['input_ids'].shape\n",
    "output_shape = batch['decision'].shape\n",
    "print(f'Input shape: {input_shape}')\n",
    "print(f'Output shape: {output_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function that converts ids into tokens\n",
    "def convert_ids_to_string(tokenizer, input):\n",
    "    return ' '.join(tokenizer.convert_ids_to_tokens(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print an example in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS] em ##bo ##diment ##s of the invention provide a method of reading and '\n",
      " 'verify ##ing a tag based on inherent disorder during a manufacturing process '\n",
      " '. the method includes using a first reader to take a first reading of an '\n",
      " 'inherent disorder feature of the tag , and using a second reader to take a '\n",
      " 'second reading of the inherent disorder feature of the tag . the method '\n",
      " 'further includes matching the first reading with the second reading , and '\n",
      " 'determining one or more acceptance criteria , wherein at least one of the '\n",
      " 'acceptance criteria is based on whether the first reading and the second '\n",
      " 'reading match within a pre ##de ##ter ##mined threshold . if the acceptance '\n",
      " 'criteria are met , then the tag is accepted , and a finger ##print for the '\n",
      " 'tag is recorded . the invention further provides a method of testing and '\n",
      " 'character ##izing a reader of inherent disorder tags during a manufacturing '\n",
      " 'process . the method includes taking a reading of a known inherent disorder '\n",
      " 'tag , using the reading to measure a characteristic of the reader , and '\n",
      " 'storing the measured characteristic for use when reading inherent disorder '\n",
      " 'tags . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] '\n",
      " '[PAD] [PAD] [PAD]')\n"
     ]
    }
   ],
   "source": [
    "# Print the example\n",
    "pprint(convert_ids_to_string(tokenizer,batch['input_ids'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('patents_shannon': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b04fe1312419ea669896150c5e116bb477c9aa479dfef38c921e951b90dadf16"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}